input
{
	#this will be uncommented when kafka communication works
	#receive the messages from kafka
    #kafka
	#{
    #    zk_connect => "localhost:2181"
    #    topic_id => "logs"
    #    reset_beginning => false 
    #}
	
	#uncomment for debugging
	#example input:
	#2014-06-10T12:10:20.123Z mcs mcs02 - WARN: We are loosing the touch. 
	stdin
	{

	}
}

filter
{
	#parse parts of message into specific fields
	#
	#to use default Banana configuration, the timestamp field has to be called 'event_timestamp'
	#example value of event_timestamp: 14-03-10T08:33:59.772Z
	#event_timestamp must exactly follow this format to be indexed in Solr
	#timestamps must be in UTC (which is indicated by Z at the end)
	#
	#example values of category: mcs, uis
	#example values of instance: mcs01, uis01
	#example values of priority: ERROR, WARN
	#
	grok
	{
		match => [ "message", "%{TIMESTAMP_ISO8601:event_timestamp} %{WORD:category} %{WORD:instance} - %{WORD:priority}: %{GREEDYDATA:data}" ]
	}
	
	#drop the message if parsing failed
	if "_grokparsefailure" in [tags]
	{
		drop
		{

		}
	}

	#force internal logstash date to match the date included in message	
	date
	{
		match => [ "event_timestamp", "YYYY-MM-dd'T'HH:mm:ss.SSS'Z'"]
	}
	#then logstash will not allow us to use this date when naming files in 
	#due to: https://logstash.jira.com/browse/LOGSTASH-1340
	#so let's just extract it from message again
	grok
	{
		match => [ "message", "%{GREEDYDATA:day}T%{GREEDYDATA}" ]
	}
	
	#remove the fields that we don't need so the solr won't error on them
	#we cannot remove filed '@timestamp', because it causes logstash to break
	#so we configured solr to receive this field and ignore it
	mutate
	{
		remove_field => [ "@version", "message", "kafka"]
    }
	
	#after dropping original message field
	#we rename 'date' field to 'message' to use default Banana configuration
	mutate
	{
		rename => [ "data", "message" ]
    }
	
	#summarise the number of messages with ERROR and WARN priority
	if ( [priority] == "ERROR" or [priority] == "WARN" )
	{
		metrics
		{
			meter => "%{priority}.%{instance}"
			add_tag => "metric"
			flush_interval => 1
		}
	}

	#adjust the statistics to fit in 'ganglia' output format
	#and calculate 'diff' field from 'count' field
	#
	#incoming events from 'metrics' filter are canceled
	#new, multiple events are produced, with tag 'parsedMetric'
	#
	if "metric" in [tags]
	{
		metricsParser
		{
			field => "count"
		}
		
	}
}

output 
{
	#uncomment for debugging
	stdout
	{
		codec => rubydebug
	}
	
	#send metrics messages from metricsParser plugin to Ganglia
	#send log messages to Solr and file system	
	if "parsedMetric" in [tags]
	{
		ganglia
		{
			codec => "plain"
			
			#this is an example of using dynamic strings in group name
			#this is currently not possible
			#I created the pull request:
			#https://github.com/elasticsearch/logstash/pull/1465
			#
			#group => "%{category}_%{instance}_events"
			#
			#please note that metrics names still have to be globally unique,
			#even if they are used in different groups			

			group => "ecloud"
			host => "239.2.11.71"
			metric => "%{instance}_%{priority}"
			metric_type => "uint32"
			port => 8649
			value => "%{diff}"
			units => "events"
		}
	}
	else
	{
		#send messages to solr
		solr_http 
		{
			codec => rubydebug
			flush_size => 100
			idle_flush_time => 1
			solr_url => "http://localhost:8080/solr/core-today"
			workers => 1
		}

		#write messages to file, one file per day per instance, separate folders for service types
		file
		{
			message_format => "%{event_timestamp} %{category} %{instance} - %{priority}: %{message}"
			#adjust the path as needed
			path => "/home/hell15user/logs/%{category}/%{instance}-%{day}.log"
		}
	}
}
